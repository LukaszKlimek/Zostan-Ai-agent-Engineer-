# ğŸ² Teoria PrawdopodobieÅ„stwa

---

## 1. Co to jest prawdopodobieÅ„stwo?

PrawdopodobieÅ„stwo to sposÃ³b na opisanie **jak bardzo moÅ¼liwe jest jakieÅ› zdarzenie**. Wszyscy intuicyjnie to znamy:

- "Jutro pewnie padaÄ‡ bÄ™dzie" â€” wysoka szansa na deszcz
- "MoÅ¼e wyrzucÄ™ szÃ³stkÄ™" â€” jakieÅ› tam szanse
- "WychodzÄ™ przez Å›cianÄ™" â€” niemoÅ¼liwe

Matematyka formalizuje te intuicje jako liczby od 0 do 1:

```
0    â€” zdarzenie niemoÅ¼liwe (na pewno siÄ™ nie stanie)
0.5  â€” zdarzenie "pÃ³Å‚ na pÃ³Å‚" (50% szansy)
1    â€” zdarzenie pewne (na pewno siÄ™ stanie)
```

MoÅ¼na teÅ¼ wyraÅ¼aÄ‡ w procentach â€” 0.7 to 70%.

Gdy mamy skoÅ„czonÄ… liczbÄ™ moÅ¼liwych wynikÃ³w, wszystkich jednakowo prawdopodobnych (uczciwa kostka, uczciwa moneta), prawdopodobieÅ„stwo obliczamy prosto:

```
                 liczba wynikÃ³w, ktÃ³re "pasujÄ…"
P(zdarzenie) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    liczba wszystkich wynikÃ³w

PrzykÅ‚ady â€” rzut kostkÄ… (wyniki: 1, 2, 3, 4, 5, 6):

P(wypadnie 6)         = 1/6 â‰ˆ 0.167     (jeden wynik pasuje z szeÅ›ciu)
P(wypadnie parzysta)  = 3/6 = 0.5       (pasujÄ…: 2, 4, 6)
P(wypadnie > 4)       = 2/6 â‰ˆ 0.333     (pasujÄ…: 5, 6)
P(wypadnie > 10)      = 0/6 = 0         (Å¼aden wynik nie pasuje)
P(wypadnie < 10)      = 6/6 = 1         (wszystkie pasujÄ…)
```

> **W AI:** Programy AI zazwyczaj nie dajÄ… odpowiedzi "tak/nie" â€” dajÄ… prawdopodobieÅ„stwo. Zamiast "to jest kot", mÃ³wiÄ… "92% szansy Å¼e to kot, 5% Å¼e pies, 3% Å¼e coÅ› innego". Taka odpowiedÅº jest znacznie bardziej uÅ¼yteczna, bo wiemy nie tylko co model myÅ›li, ale teÅ¼ jak bardzo jest tego pewny.

---

## 2. Zdarzenia i operacje na nich

**PrzestrzeÅ„ prÃ³bna** to zbiÃ³r wszystkich moÅ¼liwych wynikÃ³w danego doÅ›wiadczenia. Przy rzucie kostkÄ…: `{1, 2, 3, 4, 5, 6}`. Przy rzucie monetÄ…: `{orzeÅ‚, reszka}`.

**Zdarzenie** to dowolny podzbiÃ³r przestrzeni prÃ³bnej â€” pytamy "czy wynik naleÅ¼y do tego zbioru?".

Trzy waÅ¼ne operacje na zdarzeniach:

**Zdarzenie przeciwne** (NIE A): wszystko co NIE jest zdarzeniem A.
```
JeÅ›li A = "wypadnie parzysta" = {2, 4, 6}
To NIE-A = "wypadnie nieparzysta" = {1, 3, 5}

P(NIE A) = 1 - P(A)     â† suma musi wynosiÄ‡ 1, bo albo A, albo nie-A

P(parzysta) = 0.5   â†’   P(nieparzysta) = 1 - 0.5 = 0.5
P(szÃ³stka)  = 1/6   â†’   P(nie-szÃ³stka) = 1 - 1/6 = 5/6
```

**Suma zdarzeÅ„** (A LUB B): wynik naleÅ¼y do A, do B, lub do obu.
```
P(A lub B) = P(A) + P(B) - P(A i B)

Dlaczego odejmujemy P(A i B)? Bo wyniki naleÅ¼Ä…ce do OBU zdarzeÅ„
policzylibyÅ›my dwa razy â€” raz w P(A) i raz w P(B).

PrzykÅ‚ad: A = "parzysta" = {2,4,6},  B = "> 4" = {5,6}
P(A lub B) = 3/6 + 2/6 - 1/6 = 4/6    (wspÃ³lny wynik to {6})
Sprawdzenie: A lub B = {2, 4, 5, 6} â†’ 4 elementy â†’ 4/6 âœ…
```

**Iloczyn zdarzeÅ„** (A I B): wynik naleÅ¼y jednoczeÅ›nie do A i do B.
```
PrzykÅ‚ad: A = "parzysta",  B = "> 4"
A i B = {6}  (jedyna parzysta wiÄ™ksza od 4)
P(A i B) = 1/6
```

> **W AI:** Operacje na zdarzeniach pojawiajÄ… siÄ™ przy analizie modeli. Np. "jakie jest prawdopodobieÅ„stwo, Å¼e model siÄ™ pomyli I Å¼e bÅ‚Ä…d bÄ™dzie duÅ¼y?" to wÅ‚aÅ›nie iloczyn zdarzeÅ„. Albo przy filtrowaniu wynikÃ³w: "daj mi wyniki, ktÃ³re sÄ… pewne I naleÅ¼Ä… do kategorii X".

---

## 3. PrawdopodobieÅ„stwo warunkowe

WyobraÅº sobie takÄ… sytuacjÄ™. Rzucasz kostkÄ…, ale kolega patrzy i mÃ³wi ci "wypadÅ‚a parzysta". Teraz pytasz: jaka jest szansa Å¼e wypadÅ‚a szÃ³stka, skoro wiem Å¼e wypadÅ‚a parzysta?

To jest wÅ‚aÅ›nie **prawdopodobieÅ„stwo warunkowe** â€” prawdopodobieÅ„stwo zdarzenia A, gdy wiemy Å¼e zaszÅ‚o zdarzenie B.

```
P(A | B) = "prawdopodobieÅ„stwo A pod warunkiem B"
```

Intuicja: skoro wiemy Å¼e wypadÅ‚a parzysta, moÅ¼liwe wyniki to juÅ¼ nie {1,2,3,4,5,6} ale tylko {2,4,6}. W tym nowym, mniejszym zbiorze szÃ³stka to jeden wynik na trzy, wiÄ™c szansa wynosi 1/3.

WzÃ³r:
```
          P(A i B)
P(A|B) = â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            P(B)
```

PrzykÅ‚ad:
```
A = "szÃ³stka" = {6},         P(A) = 1/6
B = "parzysta" = {2, 4, 6},  P(B) = 3/6 = 1/2
A i B = {6},                  P(A i B) = 1/6

P(szÃ³stka | parzysta) = (1/6) / (1/2) = 1/6 Ã— 2/1 = 2/6 = 1/3 âœ…
```

PrzykÅ‚ad z Å¼ycia:
```
Wiemy, Å¼e 60% studentÃ³w zdaÅ‚o egzamin z matematyki,
a 40% zdaÅ‚o i matematykÄ™, i fizykÄ™.

P(zdaÅ‚ fizykÄ™ | zdaÅ‚ matematykÄ™) = 0.40 / 0.60 = 0.667

Czyli wÅ›rÃ³d tych co zdali matematykÄ™, 2/3 zdaÅ‚o teÅ¼ fizykÄ™.
```

> **W AI:** PrawdopodobieÅ„stwo warunkowe to fundament wielu algorytmÃ³w. Spam filter dziaÅ‚a dokÅ‚adnie tak: "jaka szansa Å¼e email jest spamem, wiedzÄ…c Å¼e zawiera sÅ‚owo WYGRAÅEÅš?". Model jÄ™zykowy (jak ChatGPT) teÅ¼ w kaÅ¼dym kroku pyta: "jakie sÅ‚owo jest najbardziej prawdopodobne jako nastÄ™pne, wiedzÄ…c jakie sÅ‚owa juÅ¼ padÅ‚y?".

---

## 4. Twierdzenie Bayesa â€” aktualizowanie wiedzy

WyobraÅº sobie lekarza. Pacjent ma objawy, ktÃ³re mogÄ… wskazywaÄ‡ na chorobÄ™ X. Lekarz wie jak czÄ™sta jest ta choroba w populacji (powiedzmy 1%). Wie teÅ¼ jak czÄ™sto objawy te pojawiajÄ… siÄ™ u chorych (90%) i jak czÄ™sto u zdrowych (10%). Chce wiedzieÄ‡: jak duÅ¼e jest prawdopodobieÅ„stwo, Å¼e *ten konkretny pacjent* jest chory?

To wÅ‚aÅ›nie robi twierdzenie Bayesa â€” Å‚Ä…czy "co wiedzieliÅ›my przed obserwacjÄ…" z "co obserwujemy" i daje "co wiemy teraz".

```
                P(B|A) Â· P(A)
P(A|B)  =  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                   P(B)
```

Gdzie:
- `P(A)` â€” co wiedzieliÅ›my o A zanim cokolwiek zobaczyliÅ›my (tu: 1% populacji jest chore)
- `P(B|A)` â€” jak prawdopodobne jest B gdy A zaszÅ‚o (90% chorych ma te objawy)
- `P(A|B)` â€” czego szukamy: prawdopodobieÅ„stwo A gdy wiemy Å¼e B zaszÅ‚o

Obliczenie:
```
P(chory) = 0.01
P(objawy | chory) = 0.90
P(objawy | zdrowy) = 0.10

P(objawy) = P(objawy|chory)Â·P(chory) + P(objawy|zdrowy)Â·P(zdrowy)
           = 0.90 Ã— 0.01  +  0.10 Ã— 0.99
           = 0.009 + 0.099 = 0.108

P(chory | objawy) = (0.90 Ã— 0.01) / 0.108 = 0.009 / 0.108 â‰ˆ 0.083
```

Wynik: tylko ~8.3% szansy na chorobÄ™ mimo objawÃ³w! Dlaczego tak maÅ‚o? Bo choroba jest rzadka (1%) i objawy pojawiajÄ… siÄ™ teÅ¼ u zdrowych. WÅ›rÃ³d 1000 osÃ³b: ~10 chorych (9 z objawami) i 990 zdrowych (99 z objawami) â€” razem 108 osÃ³b z objawami, z czego tylko 9 naprawdÄ™ chore.

Kluczowa intuicja: **Bayes mÃ³wi "zanim wyciÄ…gniesz wnioski z nowych danych, weÅº pod uwagÄ™ co wiedziaÅ‚eÅ› wczeÅ›niej".**

> **W AI:** Twierdzenie Bayesa jest fundamentem caÅ‚ej klasy algorytmÃ³w nazywanych "naiwnym Bayesem". SÄ… one uÅ¼ywane np. do klasyfikacji emaili. KaÅ¼de nowe sÅ‚owo w emailu "aktualizuje" prawdopodobieÅ„stwo Å¼e to spam â€” dokÅ‚adnie tak jak objawy aktualizowaÅ‚y prawdopodobieÅ„stwo choroby w powyÅ¼szym przykÅ‚adzie.

---

## 5. NiezaleÅ¼noÅ›Ä‡ zdarzeÅ„

Zdarzenia A i B sÄ… **niezaleÅ¼ne**, gdy wynik jednego nie wpÅ‚ywa na prawdopodobieÅ„stwo drugiego. Wiedza Å¼e A zaszÅ‚o nic nam nie mÃ³wi o B.

```
Formalny test: A i B sÄ… niezaleÅ¼ne gdy P(A i B) = P(A) Â· P(B)
```

PrzykÅ‚ady:

```
Rzut dwiema osobnymi kostkami:
  A = "pierwsza kostka: szÃ³stka"
  B = "druga kostka: szÃ³stka"
  NiezaleÅ¼ne â€” wynik pierwszej nic nie mÃ³wi o wyniku drugiej.
  P(A i B) = 1/6 Ã— 1/6 = 1/36 âœ…

Losowanie kart BEZ odkÅ‚adania:
  A = "pierwszy wylosowany: as"
  B = "drugi wylosowany: as"
  ZALEÅ»NE â€” po wylosowaniu jednego asa w talii zostaÅ‚y tylko 3 asy.
  P(B) zmienia siÄ™ w zaleÅ¼noÅ›ci od A!
```

Dlaczego to waÅ¼ne? Gdy zdarzenia sÄ… niezaleÅ¼ne, obliczenia siÄ™ upraszczajÄ… â€” mnoÅ¼ymy prawdopodobieÅ„stwa. Gdy zaleÅ¼ne â€” musimy uÅ¼ywaÄ‡ prawdopodobieÅ„stwa warunkowego.

> **W AI:** PojÄ™cie niezaleÅ¼noÅ›ci uÅ¼ywane jest przy pewnych uproszczeniach w modelach. Wspomniany "naiwny Bayes" zakÅ‚ada, Å¼e cechy danych sÄ… niezaleÅ¼ne od siebie â€” to jest wÅ‚aÅ›nie to "naiwne" zaÅ‚oÅ¼enie. W rzeczywistoÅ›ci czÄ™sto nie jest to prawda, ale uproszczenie sprawia, Å¼e algorytm jest szybki i mimo wszystko caÅ‚kiem skuteczny.

---

## 6. RozkÅ‚ady prawdopodobieÅ„stwa

Zamiast mÃ³wiÄ‡ o jednym zdarzeniu, czasem chcemy opisaÄ‡ **jak prawdopodobieÅ„stwo rozÅ‚oÅ¼one jest na wszystkie moÅ¼liwe wyniki**. Taki opis to rozkÅ‚ad prawdopodobieÅ„stwa.

**RozkÅ‚ad jednorodny (uniform)** â€” wszystkie wyniki jednakowo prawdopodobne.

```
Uczciwa kostka: kaÅ¼da Å›ciana ma szansÄ™ 1/6.
Uczciwa moneta: orzeÅ‚ i reszka majÄ… szansÄ™ 1/2.
```

**RozkÅ‚ad Bernoulliego** â€” opisuje sytuacjÄ™ z tylko dwoma wynikami: sukces (1) lub poraÅ¼ka (0). Jeden parametr `p` â€” szansa sukcesu.

```
Rzut monetÄ…: p = 0.5
StrzaÅ‚ do celu dla snajpera: p = 0.95
Rzut kostkÄ… (liczymy tylko szÃ³stkÄ™): p = 1/6
```

To prosty, ale waÅ¼ny rozkÅ‚ad â€” jest bazÄ… dla wielu bardziej zÅ‚oÅ¼onych.

**RozkÅ‚ad normalny (Gaussa)** â€” najwaÅ¼niejszy rozkÅ‚ad w statystyce. Jego wykres to charakterystyczna "krzywa dzwonowa". Opisuje wiele naturalnych zjawisk: wzrost ludzi, wyniki egzaminÃ³w, bÅ‚Ä™dy pomiarÃ³w, ceny akcji.

```
Dwa parametry:
  Î¼ (mu)    = Å›rednia â€” gdzie jest centrum dzwonu
  Ïƒ (sigma) = odchylenie standardowe â€” jak szeroki jest dzwon

        MaÅ‚y Ïƒ:                   DuÅ¼y Ïƒ:
           â–ˆ                        âˆ©
          â–ˆâ–ˆâ–ˆ                      âˆ© âˆ©
         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    âˆ©   âˆ©
        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  âˆ©     âˆ©
       â”€â”€â”€â”€â”€â”€â”€â”€â”€               â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
          Î¼                        Î¼
     (wÄ…ski, wysoki)          (szeroki, niski)
```

Bardzo uÅ¼yteczna reguÅ‚a: dla rozkÅ‚adu normalnego, okoÅ‚o 68% danych leÅ¼y w odlegÅ‚oÅ›ci jednego Ïƒ od Å›redniej, a 95% w odlegÅ‚oÅ›ci dwÃ³ch Ïƒ.

```
Wzrost PolakÃ³w: Î¼ = 175 cm, Ïƒ = 7 cm
â†’ 68% PolakÃ³w ma wzrost miÄ™dzy 168 a 182 cm
â†’ 95% PolakÃ³w ma wzrost miÄ™dzy 161 a 189 cm
```

> **W AI:** RozkÅ‚ad normalny pojawia siÄ™ wszÄ™dzie. BÅ‚Ä™dy predykcji modeli czÄ™sto majÄ… rozkÅ‚ad normalny. Przy inicjalizacji modelu (ustawianiu startowych wartoÅ›ci parametrÃ³w) czÄ™sto korzysta siÄ™ z losowania z rozkÅ‚adu normalnego. Rozumienie ksztaÅ‚tu tego rozkÅ‚adu i co oznaczajÄ… Î¼ i Ïƒ to niezbÄ™dna podstawa.

---

## 7. WartoÅ›Ä‡ oczekiwana â€” "Å›rednia w dÅ‚ugim biegu"

WyobraÅº sobie, Å¼e wielokrotnie powtarzasz jakieÅ› doÅ›wiadczenie losowe (rzucasz kostkÄ… setki razy). **WartoÅ›Ä‡ oczekiwana** to liczba, wokÃ³Å‚ ktÃ³rej bÄ™dÄ… skupiaÄ‡ siÄ™ twoje wyniki â€” "dÅ‚ugookresowa Å›rednia".

```
E[X] = xâ‚Â·P(X=xâ‚) + xâ‚‚Â·P(X=xâ‚‚) + ...
     = suma: (wartoÅ›Ä‡) Ã— (prawdopodobieÅ„stwo tej wartoÅ›ci)
```

PrzykÅ‚ad â€” rzut kostkÄ…:

```
E[X] = 1Â·(1/6) + 2Â·(1/6) + 3Â·(1/6) + 4Â·(1/6) + 5Â·(1/6) + 6Â·(1/6)
     = (1 + 2 + 3 + 4 + 5 + 6) / 6
     = 21 / 6 = 3.5
```

Nikt nie "wyrzuci 3.5", ale po wielu rzutach Å›redni wynik zbliÅ¼y siÄ™ do 3.5. To wÅ‚aÅ›nie wartoÅ›Ä‡ oczekiwana.

Bardziej Å¼yciowy przykÅ‚ad â€” loteria:

```
Kupujesz bilet za 5 zÅ‚.
Z prawdopodobieÅ„stwem 0.001 wygrywasz 1000 zÅ‚.
Z prawdopodobieÅ„stwem 0.999 nie wygrywasz nic.

E[wygrana] = 1000 Ã— 0.001 + 0 Ã— 0.999 = 1 zÅ‚

Spodziewana wygrana to tylko 1 zÅ‚, a bilet kosztuje 5 zÅ‚.
DÅ‚ugookresowo tracisz 4 zÅ‚ na kaÅ¼dym bilecie.
```

> **W AI:** WartoÅ›Ä‡ oczekiwana pojawia siÄ™ przy obliczaniu "spodziewanego bÅ‚Ä™du" modelu â€” jak duÅ¼y bÅ‚Ä…d robi model Å›rednio, biorÄ…c pod uwagÄ™ caÅ‚e spektrum moÅ¼liwych przykÅ‚adÃ³w? To jest podstawa do budowania funkcji, ktÃ³re mierzymy i minimalizujemy podczas trenowania.

---

## âœ… SprawdÅº siÄ™

1. Rzucasz kostkÄ…. Jakie jest prawdopodobieÅ„stwo Å¼e wypadnie parzysta LUB wiÄ™ksza niÅ¼ 4?
2. Co to znaczy `P(A|B) = 0.8`?
3. Oblicz wartoÅ›Ä‡ oczekiwanÄ… rzutu monetÄ…, gdzie orzeÅ‚ = 10 pkt, reszka = 0 pkt, kaÅ¼dy z prawdopodobieÅ„stwem 0.5.
4. WyjaÅ›nij wÅ‚asnymi sÅ‚owami czym rÃ³Å¼ni siÄ™ rozkÅ‚ad normalny o maÅ‚ym Ïƒ od tego o duÅ¼ym Ïƒ.

<details>
<summary>Odpowiedzi</summary>

1. Parzyste: {2,4,6}, wiÄ™ksze niÅ¼ 4: {5,6}, czÄ™Å›Ä‡ wspÃ³lna: {6}
   P(A lub B) = 3/6 + 2/6 - 1/6 = 4/6 â‰ˆ 0.667

2. PrawdopodobieÅ„stwo zdarzenia A wynosi 80%, **pod warunkiem Å¼e wiemy Å¼e B juÅ¼ zaszÅ‚o**. Wiedza o B zwiÄ™ksza lub zmniejsza nasze oczekiwania co do A.

3. E[X] = 10 Ã— 0.5 + 0 Ã— 0.5 = 5 pkt

4. MaÅ‚y Ïƒ: dane skupione blisko Å›redniej, krzywa wÄ…ska i wysoka. DuÅ¼y Ïƒ: dane mocno rozrzucone, krzywa szeroka i niska.

</details>
