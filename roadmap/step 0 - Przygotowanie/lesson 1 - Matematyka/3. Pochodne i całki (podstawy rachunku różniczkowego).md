# ğŸ“‰ Pochodne i CaÅ‚ki

---

## 1. Co mierzy pochodna? â€” intuicja

Zacznijmy od pytania z Å¼ycia. Jedziesz samochodem. W pewnym momencie patrzysz na prÄ™dkoÅ›ciomierz i widzisz 90 km/h. Co to wÅ‚aÅ›ciwie oznacza?

To nie jest prÄ™dkoÅ›Ä‡ "z ostatniej godziny" ani "z ostatniej minuty". To jest prÄ™dkoÅ›Ä‡ **w tej chwili** â€” jak szybko zmienia siÄ™ twoja pozycja wÅ‚aÅ›nie teraz, w tym konkretnym momencie.

WÅ‚aÅ›nie to mierzy pochodna. **Pochodna funkcji w danym punkcie to tempo, w jakim ta funkcja zmienia siÄ™ w tym punkcie.** Innymi sÅ‚owy â€” jak szybko wartoÅ›Ä‡ funkcji roÅ›nie lub maleje.

Kilka codziennych przykÅ‚adÃ³w tego samego pomysÅ‚u:

- Termometr pokazuje 20Â°C, ale temperatura roÅ›nie 2 stopnie na godzinÄ™ â€” to pochodna temperatury wzglÄ™dem czasu.
- Saldo konta wynosi 5000 zÅ‚, ale roÅ›nie 100 zÅ‚ dziennie â€” to pochodna salda wzglÄ™dem czasu.
- Wspinasz siÄ™ na gÃ³rÄ™ â€” jak bardzo strome jest zbocze *w tym miejscu* â€” to pochodna wysokoÅ›ci wzglÄ™dem odlegÅ‚oÅ›ci.

**Pochodna nie mÃ³wi jaka jest wartoÅ›Ä‡ funkcji. MÃ³wi jak szybko ta wartoÅ›Ä‡ siÄ™ zmienia.**

> **W AI:** Program AI podczas nauki chce zmniejszaÄ‡ swÃ³j bÅ‚Ä…d. Å»eby wiedzieÄ‡ w ktÃ³rÄ… stronÄ™ zmieniaÄ‡ swoje parametry (liczby wewnÄ™trzne), musi wiedzieÄ‡ jak zmiana kaÅ¼dego parametru wpÅ‚ywa na bÅ‚Ä…d. OdpowiedÅº na to pytanie daje wÅ‚aÅ›nie pochodna. To absolutny fundament uczenia siÄ™ maszyn.

---

## 2. Geometryczna intuicja â€” nachylenie stycznej

Narysuj sobie dowolnÄ… krzywÄ…. Wybierz na niej jeden punkt. PrzyÅ‚Ã³Å¼ do tego punktu linijkÄ™ tak, Å¼eby dotykaÅ‚a krzywej tylko w tym jednym miejscu â€” to jest **styczna**.

Im bardziej "pod gÃ³rÄ™" biegnie ta styczna, tym wiÄ™ksza pochodna w tym punkcie. JeÅ›li styczna jest pozioma, pochodna wynosi zero. JeÅ›li styczna biegnie "w dÃ³Å‚", pochodna jest ujemna.

```
y
â”‚              /â† styczna: mocno stroma = duÅ¼a pochodna
â”‚           __/
â”‚        __/
â”‚     __/ â† funkcja
â”‚  __/
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ x

y
â”‚
â”‚    ____â”€â”€â”€â”€â”€â”€â”€â”€   â† styczna pozioma = pochodna = 0 (tu jest minimum lub maksimum)
â”‚ __/
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ x
```

To prowadzi do waÅ¼nej obserwacji: **w punktach minimum i maksimum funkcji, styczna jest pozioma, a pochodna wynosi zero**. Znaczy to, Å¼e jeÅ›li chcemy znaleÅºÄ‡ gdzie funkcja osiÄ…ga swoje minimum lub maksimum, szukamy miejsc, gdzie pochodna = 0.

> **W AI:** Minimalizacja bÅ‚Ä™du modelu to szukanie punktu, w ktÃ³rym pochodna wynosi zero. DokÅ‚adnie tak jak szukamy dna paraboli â€” w dnie styczna jest pozioma. W praktyce uÅ¼ywa siÄ™ do tego algorytmu zwanego gradient descent, o ktÃ³rym za chwilÄ™.

---

## 3. Jak obliczaÄ‡ pochodne â€” podstawowe reguÅ‚y

Zamiast za kaÅ¼dym razem liczyÄ‡ z definicji (co jest moÅ¼liwe, ale Å¼mudne), mamy gotowe reguÅ‚y.

**ReguÅ‚a 1: Pochodna staÅ‚ej wynosi 0**

JeÅ›li funkcja siÄ™ nie zmienia (jest staÅ‚a), to tempo jej zmiany = 0. Logiczne.

```
f(x) = 7   â†’   f'(x) = 0
f(x) = -3  â†’   f'(x) = 0
```

Apostrof po literze (`f'`) to standardowy zapis pochodnej.

**ReguÅ‚a 2: Pochodna potÄ™gi â€” "zepchnij wykÅ‚adnik na dÃ³Å‚ i zmniejsz o 1"**

```
f(x) = xâ¿   â†’   f'(x) = n Â· xâ¿â»Â¹

f(x) = xÂ²   â†’   f'(x) = 2x
f(x) = xÂ³   â†’   f'(x) = 3xÂ²
f(x) = x    â†’   f'(x) = 1      (bo x = xÂ¹, wiÄ™c pochodna = 1 Â· xâ° = 1)
```

**ReguÅ‚a 3: Pochodna sumy â€” liczymy kaÅ¼dy skÅ‚adnik osobno**

```
f(x) = xÂ³ + 5xÂ² - 2x + 7

f'(x) = 3xÂ²  +  10x  -  2  +  0
        â†‘         â†‘      â†‘     â†‘
       (xÂ³)'   (5xÂ²)'  (2x)' (7)'
```

**ReguÅ‚a 4: ReguÅ‚a Å‚aÅ„cuchowa â€” dla zÅ‚oÅ¼enia funkcji**

PamiÄ™tasz zÅ‚oÅ¼enie funkcji z poprzedniej lekcji? `f(g(x))` to dwie funkcje jedna w drugiej. Pochodna takiego zÅ‚oÅ¼enia to:

```
[f(g(x))]' = f'(g(x)) Â· g'(x)
```

W sÅ‚owach: pochodna zewnÄ™trznej funkcji (liczona dla wartoÅ›ci wewnÄ™trznej) pomnoÅ¼ona przez pochodnÄ… wewnÄ™trznej funkcji.

PrzykÅ‚ad:

```
h(x) = (xÂ² + 1)Â³

ZewnÄ™trzna: "coÅ› do trzeciej", f(u) = uÂ³  â†’  f'(u) = 3uÂ²
WewnÄ™trzna: "xÂ² + 1",         g(x) = xÂ²+1 â†’  g'(x) = 2x

h'(x) = 3(xÂ² + 1)Â² Â· 2x = 6x(xÂ² + 1)Â²
```

Jeszcze jeden przykÅ‚ad, Å¼eby to wryÄ‡:

```
h(x) = âˆš(3x + 5) = (3x + 5)^(1/2)

ZewnÄ™trzna: "coÅ› do 1/2", f(u) = u^(1/2)  â†’  f'(u) = (1/2)u^(-1/2) = 1/(2âˆšu)
WewnÄ™trzna: "3x + 5",     g(x) = 3x + 5   â†’  g'(x) = 3

h'(x) = 1/(2âˆš(3x+5)) Â· 3 = 3 / (2âˆš(3x+5))
```

> **W AI:** ReguÅ‚a Å‚aÅ„cuchowa jest kluczem do tego jak programy AI obliczajÄ… swÃ³j bÅ‚Ä…d "wstecz" przez wiele warstw obliczeÅ„. MÃ³wiÄ…c obrazowo: jeÅ›li model to seria funkcji zÅ‚oÅ¼ona jedna po drugiej, to Å¼eby wiedzieÄ‡ jak zmieniÄ‡ pierwszÄ… funkcjÄ™, trzeba "przejÅ›Ä‡ przez" wszystkie kolejne reguÅ‚y Å‚aÅ„cuchowÄ…. To siÄ™ nazywa backpropagation â€” ale szczegÃ³Å‚y poznasz w dalszych krokach kursu, kiedy bÄ™dziesz siÄ™ uczyÄ‡ o sieciach neuronowych.

---

## 4. Gradient â€” pochodna dla wielu zmiennych

Do tej pory mieliÅ›my funkcje jednej zmiennej: `f(x)`. Ale co jeÅ›li funkcja ma wiele zmiennych? Na przykÅ‚ad `f(x, y) = xÂ² + 3xy + yÂ²`?

Wtedy zamiast jednej pochodnej mamy **pochodne czÄ…stkowe** â€” jednÄ… dla kaÅ¼dej zmiennej. Oblicza siÄ™ je tak: bierzesz pochodnÄ… po jednej zmiennej i traktujesz pozostaÅ‚e jak staÅ‚e liczby.

```
f(x, y) = xÂ² + 3xy + yÂ²

Pochodna po x (y jak staÅ‚a):    âˆ‚f/âˆ‚x = 2x + 3y
Pochodna po y (x jak staÅ‚a):    âˆ‚f/âˆ‚y = 3x + 2y
```

Symbol `âˆ‚` (czytaj "del" lub "czÄ…stkowa d") oznacza pochodnÄ… czÄ…stkowÄ….

**Gradient** to wektor zÅ‚oÅ¼ony ze wszystkich pochodnych czÄ…stkowych:

```
âˆ‡f = [âˆ‚f/âˆ‚x, âˆ‚f/âˆ‚y] = [2x + 3y,  3x + 2y]
```

Intuicja gradientu: wyobraÅº sobie mapÄ™ wysokoÅ›ci gÃ³rskiego terenu. Stoisz w pewnym punkcie. Gradient to strzaÅ‚ka wskazujÄ…ca kierunek, w ktÃ³rym teren najbardziej stromo idzie **w gÃ³rÄ™**. OdwrotnoÅ›Ä‡ gradientu (minus gradient) wskazuje kierunek najszybszego **zejÅ›cia w dÃ³Å‚**.

> **W AI:** Gradient to serce uczenia maszynowego. Model AI ma dziesiÄ…tki tysiÄ™cy (lub miliardy) parametrÃ³w â€” liczb, ktÃ³re regulujÄ… jego zachowanie. Gradient bÅ‚Ä™du mÃ³wi modelowi: "zmieÅ„ te parametry w tÄ™ stronÄ™, Å¼eby bÅ‚Ä…d siÄ™ zmniejszyÅ‚". Algorytm ktÃ³ry to robi nazywa siÄ™ gradient descent â€” model w kaÅ¼dym kroku przesuwa swoje parametry odrobinÄ™ w kierunku wskazanym przez ujemny gradient, zmniejszajÄ…c bÅ‚Ä…d krok po kroku.

---

## 5. Znajdowanie minimum i maksimum

Wiemy juÅ¼ Å¼e w punktach minimum i maksimum pochodna wynosi zero. Teraz moÅ¼emy to wykorzystaÄ‡.

**Procedura:** oblicz pochodnÄ…, ustaw jÄ… rÃ³wnÄ… zero, znajdÅº x.

PrzykÅ‚ad:

```
ZnajdÅº minimum funkcji f(x) = xÂ² - 6x + 10

Krok 1: Oblicz pochodnÄ…
  f'(x) = 2x - 6

Krok 2: Ustaw f'(x) = 0
  2x - 6 = 0
  2x = 6
  x = 3

Krok 3: Oblicz wartoÅ›Ä‡ funkcji w x = 3
  f(3) = 9 - 18 + 10 = 1

Minimum wynosi 1 i leÅ¼y w punkcie x = 3.
```

Jak sprawdziÄ‡ czy to minimum czy maksimum? Prosta metoda: sprawdÅº wartoÅ›Ä‡ funkcji po obu stronach.

```
f(2) = 4 - 12 + 10 = 2   > 1  â† wyÅ¼ej niÅ¼ minimum
f(4) = 16 - 24 + 10 = 2  > 1  â† wyÅ¼ej niÅ¼ minimum

Potwierdzenie: x = 3 to minimum. âœ…
```

> **W AI:** CaÅ‚y algorytm uczenia modelu AI to nic innego jak szukanie minimum funkcji bÅ‚Ä™du. RÃ³Å¼nica wzglÄ™dem powyÅ¼szego przykÅ‚adu jest taka, Å¼e funkcja bÅ‚Ä™du nie ma jednej zmiennej tylko miliony (kaÅ¼dy parametr modelu to zmienna). Nie da siÄ™ rozwiÄ…zaÄ‡ takiego ukÅ‚adu analitycznie, wiÄ™c stosuje siÄ™ podejÅ›cie iteracyjne: gradient descent.

---

## 6. CaÅ‚ki â€” odwrotnoÅ›Ä‡ pochodnej

CaÅ‚ka to operacja odwrotna do rÃ³Å¼niczkowania. JeÅ›li pochodna odpowiada na pytanie "jak szybko funkcja siÄ™ zmienia?", to caÅ‚ka odpowiada na pytanie "jaka funkcja zmieniaÅ‚a siÄ™ w ten sposÃ³b?".

Poza tym algebraicznym sensem, caÅ‚ka ma piÄ™knÄ… interpretacjÄ™ geometrycznÄ…: **caÅ‚ka oznaczona to pole powierzchni pod wykresem funkcji**.

```
y
â”‚        *
â”‚      *â–ˆâ”‚â–ˆ
â”‚    *â–ˆâ”‚â–ˆâ”‚â–ˆ
â”‚   *â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚
â”‚  *â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚â–ˆâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ x
   a           b

Zacieniowany obszar = âˆ«â‚áµ‡ f(x) dx
```

Symbol `âˆ«` (wydÅ‚uÅ¼ona litera S od "summa") oznacza caÅ‚kÄ™. `a` i `b` to granice â€” czyli od jakiego do jakiego x liczymy pole.

Obliczanie caÅ‚ki: szukasz funkcji, ktÃ³rej pochodna to ta, ktÃ³rÄ… caÅ‚kujesz.

```
JeÅ›li pochodna z xÂ³ to 3xÂ²,
to caÅ‚ka z 3xÂ² to xÂ³ (plus staÅ‚a C, bo pochodna staÅ‚ej = 0)

âˆ« 3xÂ² dx = xÂ³ + C

Kilka waÅ¼nych przykÅ‚adÃ³w:
  âˆ« 1 dx  = x + C
  âˆ« x dx  = xÂ²/2 + C
  âˆ« xÂ² dx = xÂ³/3 + C
```

Obliczanie pola (caÅ‚ka oznaczona) â€” korzystasz ze wzoru `F(b) - F(a)`:

```
Ile wynosi pole pod wykresem f(x) = xÂ² od x = 1 do x = 3?

Funkcja pierwotna: F(x) = xÂ³/3

âˆ«â‚Â³ xÂ² dx = F(3) - F(1) = 27/3 - 1/3 = 9 - 1/3 â‰ˆ 8.67
```

> **W AI:** CaÅ‚ki pojawiajÄ… siÄ™ w teorii stojÄ…cej za modelami probabilistycznymi â€” kiedy trzeba obliczyÄ‡ "Å›redniÄ… oczekiwanÄ… wartoÅ›Ä‡" dla zmiennych ciÄ…gÅ‚ych, np. jaka jest spodziewana wartoÅ›Ä‡ pomiaru gdy wiemy tylko jak jest rozÅ‚oÅ¼one prawdopodobieÅ„stwo. Na etapie kroku 0 wystarczy intuicja: caÅ‚ka = pole pod wykresem. SzczegÃ³Å‚y przyjdÄ… przy gÅ‚Ä™bszej nauce.

---

## âœ… SprawdÅº siÄ™

1. Oblicz pochodnÄ… `f(x) = 4xÂ³ - 6xÂ² + 2`.
2. UÅ¼ywajÄ…c reguÅ‚y Å‚aÅ„cuchowej, oblicz pochodnÄ… `h(x) = (2x + 1)â´`.
3. ZnajdÅº minimum funkcji `f(x) = xÂ² - 4x + 7`.
4. Oblicz `âˆ« (2x + 3) dx`.

<details>
<summary>Odpowiedzi</summary>

1. `f'(x) = 12xÂ² - 12x`

2. ZewnÄ™trzna: `(uâ´)' = 4uÂ³`, wewnÄ™trzna: `(2x+1)' = 2`
   Wynik: `h'(x) = 4(2x+1)Â³ Â· 2 = 8(2x+1)Â³`

3. `f'(x) = 2x - 4 = 0` â†’ `x = 2`, minimum: `f(2) = 4 - 8 + 7 = 3`

4. `xÂ² + 3x + C`

</details>
